{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 02. Train U-Net\n",
                "This notebook trains the U-Net model on the COCO dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "from tqdm import tqdm\n",
                "\n",
                "sys.path.append('../src')\n",
                "from dataset import COCOSegmentationDataset, get_training_augmentations, get_validation_augmentations\n",
                "from model import UNet\n",
                "from utils import visualize"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "def dice_loss(pred_logits, target, smooth = 1.):\n",
                "    # Apply sigmoid to logits to get probabilities for Dice Loss\n",
                "    pred = torch.sigmoid(pred_logits)\n",
                "    \n",
                "    pred = pred.contiguous()\n",
                "    target = target.contiguous()\n",
                "\n",
                "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
                "    \n",
                "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
                "    \n",
                "    return loss.mean()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n"
                    ]
                }
            ],
            "source": [
                "# Hyperparameters\n",
                "LEARNING_RATE = 1e-4\n",
                "BATCH_SIZE = 4 # Adjust based on GPU memory\n",
                "NUM_EPOCHS = 10\n",
                "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "TRAIN_DIR = '../data/coco2017/train2017'\n",
                "TRAIN_ANN = '../data/coco2017/annotations/instances_train2017.json'\n",
                "VAL_DIR = '../data/coco2017/val2017'\n",
                "VAL_ANN = '../data/coco2017/annotations/instances_val2017.json'\n",
                "\n",
                "print(f\"Using device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/abood/Documents/sync/Education/ENSA/ENSA_M3/Computer Vision/Project_coco/.venv/lib/python3.12/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
                        "  original_init(self, **validated_kwargs)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "loading annotations into memory...\n",
                        "Done (t=14.06s)\n",
                        "creating index...\n",
                        "index created!\n",
                        "loading annotations into memory...\n",
                        "Done (t=0.62s)\n",
                        "creating index...\n",
                        "index created!\n",
                        "Train size: 117266, Val size: 4952\n"
                    ]
                }
            ],
            "source": [
                "# Dataset & DataLoader\n",
                "# Using separate datasets for Train and Val as per folder structure\n",
                "train_dataset = COCOSegmentationDataset(TRAIN_DIR, TRAIN_ANN, transforms=get_training_augmentations())\n",
                "val_dataset = COCOSegmentationDataset(VAL_DIR, VAL_ANN, transforms=get_validation_augmentations())\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
                "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
                "\n",
                "print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model Setup\n",
                "model = UNet(n_channels=3, n_classes=1).to(DEVICE)\n",
                "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
                "# BCEWithLogitsLoss combines Sigmoid and BCELoss for numerical stability\n",
                "bce_criterion = nn.BCEWithLogitsLoss()\n",
                "\n",
                "# Updated for newer PyTorch versions\n",
                "scaler = torch.amp.GradScaler('cuda') # For Mixed Precision"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch [1/10]: 100%|██████████| 29317/29317 [2:56:58<00:00,  2.76it/s, loss=1.09]   \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1 Average Loss: 1.3099\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch [2/10]: 100%|██████████| 29317/29317 [2:58:13<00:00,  2.74it/s, loss=1.16]   \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 2 Average Loss: 1.2838\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch [3/10]: 100%|██████████| 29317/29317 [2:57:01<00:00,  2.76it/s, loss=1.03]   \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 3 Average Loss: 1.2765\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch [4/10]: 100%|██████████| 29317/29317 [2:54:42<00:00,  2.80it/s, loss=1.28]   \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 4 Average Loss: 1.2718\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch [5/10]: 100%|██████████| 29317/29317 [2:41:55<00:00,  3.02it/s, loss=1.04]   \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 5 Average Loss: 1.2685\n",
                        "Model saved!\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch [6/10]:  29%|██▊       | 8417/29317 [45:59<1:54:12,  3.05it/s, loss=1.32] \n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m optimizer.zero_grad()\n\u001b[32m     19\u001b[39m scaler.scale(loss).backward()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m scaler.update()\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Update tqdm\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/sync/Education/ENSA/ENSA_M3/Computer Vision/Project_coco/.venv/lib/python3.12/site-packages/torch/amp/grad_scaler.py:462\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    456\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m, (\n\u001b[32m    459\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    460\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/sync/Education/ENSA/ENSA_M3/Computer Vision/Project_coco/.venv/lib/python3.12/site-packages/torch/amp/grad_scaler.py:356\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    349\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    350\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    353\u001b[39m     **kwargs: Any,\n\u001b[32m    354\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    355\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf_per_device\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    357\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/sync/Education/ENSA/ENSA_M3/Computer Vision/Project_coco/.venv/lib/python3.12/site-packages/torch/amp/grad_scaler.py:356\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    349\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    350\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    353\u001b[39m     **kwargs: Any,\n\u001b[32m    354\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    355\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    357\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
                        "\u001b[31mKeyboardInterrupt\u001b[39m: "
                    ]
                }
            ],
            "source": [
                "# Training Loop\n",
                "for epoch in range(NUM_EPOCHS):\n",
                "    model.train()\n",
                "    loop = tqdm(train_loader, leave=True)\n",
                "    train_loss = 0\n",
                "    \n",
                "    for idx, (data, targets) in enumerate(loop):\n",
                "        data = data.to(DEVICE)\n",
                "        targets = targets.to(DEVICE)\n",
                "\n",
                "        # Forward\n",
                "        with torch.amp.autocast('cuda'):\n",
                "            logits = model(data)\n",
                "            # Loss calculated on logits\n",
                "            loss = bce_criterion(logits, targets) + dice_loss(logits, targets)\n",
                "\n",
                "        # Backward\n",
                "        optimizer.zero_grad()\n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "\n",
                "        # Update tqdm\n",
                "        train_loss += loss.item()\n",
                "        loop.set_description(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
                "        loop.set_postfix(loss=loss.item())\n",
                "    \n",
                "    print(f\"Epoch {epoch+1} Average Loss: {train_loss/len(train_loader):.4f}\")\n",
                "    \n",
                "    # Save Model\n",
                "    if (epoch + 1) % 5 == 0:\n",
                "        torch.save(model.state_dict(), f\"unet_coco_epoch_{epoch+1}.pth\")\n",
                "        print(\"Model saved!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "checkpoint = {\n",
                "    'epoch': epoch + 1,\n",
                "    'model_state_dict': model.state_dict(),\n",
                "    'optimizer_state_dict': optimizer.state_dict(), # Saves momentum\n",
                "    'loss': train_loss,\n",
                "}\n",
                "torch.save(checkpoint, \"full_checkpoint.pth\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
